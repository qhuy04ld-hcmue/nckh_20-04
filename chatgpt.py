import pandas as pd
import torch
from datasets import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments,
)
from sklearn.metrics import accuracy_score

# === STEP 1: Äá»ŒC FILE ===
def load_data():
    with open("ly.txt", "r", encoding="utf-8") as f:
        ly_texts = [line.strip() for line in f.readlines() if line.strip()]
    with open("hoa.txt", "r", encoding="utf-8") as f:
        hoa_texts = [line.strip() for line in f.readlines() if line.strip()]
    
    df = pd.DataFrame({
        "text": ly_texts + hoa_texts,
        "label": [0] * len(ly_texts) + [1] * len(hoa_texts)  # 0 = Váº­t lÃ½, 1 = HÃ³a há»c
    })
    df = df.sample(frac=1).reset_index(drop=True)  # Shuffle
    return df

# === STEP 2: TOKENIZE ===
def preprocess_data(df):
    dataset = Dataset.from_pandas(df)
    tokenizer = AutoTokenizer.from_pretrained("vinai/phobert-base", use_fast=False)

    def preprocess(example):
        return tokenizer(example["text"], padding="max_length", truncation=True, max_length=128)

    dataset = dataset.map(preprocess)
    dataset = dataset.train_test_split(test_size=0.2)
    return dataset, tokenizer

# === STEP 3: TÃNH Äá»˜ CHÃNH XÃC ===
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = logits.argmax(axis=1)
    acc = accuracy_score(labels, preds)
    return {"accuracy": acc}

# === STEP 4: HUáº¤N LUYá»†N MÃ” HÃŒNH ===
def train_model(dataset, tokenizer):
    model = AutoModelForSequenceClassification.from_pretrained("vinai/phobert-base", num_labels=2)

    training_args = TrainingArguments(
        output_dir="./results",
        learning_rate=2e-5,
        per_device_train_batch_size=8,
        num_train_epochs=4,
        weight_decay=0.01,
        logging_dir="./logs",  # LÆ°u log
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset["train"],
        eval_dataset=dataset["test"],
        tokenizer=tokenizer,
        compute_metrics=compute_metrics,  # TÃ­nh Ä‘á»™ chÃ­nh xÃ¡c
    )

    trainer.train()
    return model, tokenizer, trainer


# === STEP 5: Dá»° ÄOÃN ===
def predict(text, model, tokenizer):
    model.eval()
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=128)
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
        label = torch.argmax(probs).item()
        confidence = probs[0][label].item()
    return ("HÃ³a há»c" if label == 1 else "Váº­t lÃ½", round(confidence, 4))

# === MAIN FUNCTION ===
def main():
    print("ğŸš€ Äang táº£i dá»¯ liá»‡u tá»« ly.txt vÃ  hoa.txt...")
    df = load_data()
    print(f"âœ… Dá»¯ liá»‡u gá»“m {len(df)} dÃ²ng.")

    print("ğŸ”„ Tiá»n xá»­ lÃ½ vÃ  tokenizing...")
    dataset, tokenizer = preprocess_data(df)

    print("ğŸ‹ï¸â€â™‚ï¸ Äang huáº¥n luyá»‡n mÃ´ hÃ¬nh PhoBERT...")
    model, tokenizer, trainer = train_model(dataset, tokenizer)

    print("ğŸ“Š ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh:")
    metrics = trainer.evaluate()
    print(metrics)

    while True:
        text = input("\nNháº­p cÃ¢u cáº§n phÃ¢n loáº¡i ('q' Ä‘á»ƒ thoÃ¡t): ")
        if text.lower() == "q":
            break
        label, confidence = predict(text, model, tokenizer)
        print(f"â¡ï¸ Káº¿t quáº£: {label} (Ä‘á»™ tin cáº­y: {confidence*100:.2f}%)")

if __name__ == "__main__":
    main()
